# Description

Every open dataset, which does not require an external account for download, can be downloaded as an archive
through the frontend of the CONP portal. Those dataset archives are being created in the CONP archiver VM that
has a cronjob running to automatically update the dataset archives to the latest content of the dataset.


# VM information

The VM is accessible though the MCIN login VM only. 

CONP archiver VM information:
- user: `conp_admin`
- host: `archiver.conp.ca -p 4723`
- password: please contact Emmet O'Brien or CÃ©cile Madjar for the password

VM downtime for backup: 
- every Wednesdays at 4:45AM.


# Archiver code

The script that generates dataset archives is located in `/data/conp-dataset/scripts` and called `auto_archive.py`.
**Note**: given that some datasets are very large to download and processing can be long, it is recommended to run the
script via `screen` to keep the job running if connection to the VM is lost for some reason.

Usage:
```bash
(base) conp_admin@archiver:/data/conp-dataset$ PYTHONPATH=$PWD python scripts/auto_archive.py -h
usage: auto_archive.py [-h] [--out_dir OUT_DIR] [--max-size MAX_SIZE] [--all | --dataset DATASET [DATASET ...]]

Archiver for the CONP-datasets.

optional arguments:
  -h, --help            show this help message and exit
  --out_dir OUT_DIR, -o OUT_DIR
                        Path to store the archived datasets.
  --max-size MAX_SIZE   Maximum size of dataset to archive in GB.
  --all                 Archive all the datasets rather than those modified since the last time.
  --dataset DATASET [DATASET ...], -d DATASET [DATASET ...]
                        Restrict the archive to the specified dataset paths.

Example:
    PYTHONPATH=$PWD python scripts/auto_archive.py <out_dir>

```

Archives generated by the script are all located in the `/data/archives` directory on the VM. Naming convention for
the archives is `<DatasetName>_version-<DatasetVersion>.tar.gz`.


### Python installation and virtual environment

Python has been installed using [Miniconda](https://docs.conda.io/en/latest/miniconda.html). 

The virtual environment used for the archiver script is the default `base` Miniconda environment and is running 
Python version 3.9.5 with the required libraries installed through `pip`.


### Regenerating an archive for a particular dataset

Sometimes, it might be useful to regenerate the archive of a particular dataset (for example if a cronjob did not 
regenerate an archive for an updated dataset or if something went wrong during the recreation of the archive).
In order to do so, go to the `/data/conp-dataset` directory and run:

```bash
PYTHONPATH=$PWD python scripts/auto_archive.py --out_dir /data/archives/ --dataset projects/<DATASET_DIRECTORY_NAME>
```


### Updating archives for datasets that have been modified since the last time the archiver script was run

To run the archiver script to only regenerate archives for datasets that have been modified since the last time the
archiver script was run, go to the `/data/conp-dataset` directory and run:

```bash
PYTHONPATH=$PWD python scripts/auto_archive.py --out_dir /data/archives/
```

Note: this is the default behaviour run daily by the cronjob setup on the VM


### Current cronjob setup

A daily cronjob has been set up to regenerate archives for datasets that have been modified or generate a new 
archive for new datasets added to the [conp-dataset](https://github.com/CONP-PCNO/conp-dataset.git) repository.

The daily cronjob is set up to run the `~/cronscript/archiver.sh` script that was developed for the crontab. When
the archiver is run through this script a `.archiving` text file is created in `/data/conp-dataset/` so that
another cronjob does not start another archiving process while the previous one is not finished yet.

A copy of the crontab is also available in the `~/cronscript/` directory. It is always a good idea to ensure a 
backup of the crontab is available in case of accidental deletion of the crontab (with the `r` option) instead 
of editing it (with the `e` option).

To generate a new back up of the crontab, run `crontab -l` and pipe it into a new file (for example
`crontab -l > crontab_bkp_date`)

Once the cronjob has finished running, an email will be sent to the list of emails present in the `MAILTO` variable 
of the crontab.

**Note**: when datasets are very large, it can happen that the `datalad get` part takes more than a week to download
the dataset. In those cases, it is advisable to submit a ticket to Redmine to temporarily remove the weekly restart
of the VM backups.


# Transfer of the new archives to the portal

The transfer of the new archives to the portal is still a manual intervention to avoid automatically syncing corrupted
archives to the portal. Once the new created archive(s) have been looked at to ensure they were properly created
(for example: no `datalad get` error in the emailed cronjob output; generated file .tar.gz cannot be untarred...),
the new archives can be rsynced to the portal.

### Rsyncing archives on the portal

The `/data/` directory of the `archiver.conp.ca` VM is mounted onto `portal.conp.ca` as `/data-archiver` to facilitate 
the rsync process on the portal VM.

Once logged to the `portal.conp.ca` VM as `conp-admin`, run the following command:

```bash
rsync -av /data-archiver/archives/ /data/not_backed_up/conp-portal/data/.cache/
```

